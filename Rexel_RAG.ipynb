{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5dee0c-10d8-4394-8b23-16df1ff7ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import faiss\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbf4debf-a3d3-4403-9282-dc5d62f3ed0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-ranker model 'cross-encoder/ms-marco-MiniLM-L-6-v2' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "OPENAI_API_KEY = 'knvh5u5rfuyg6877gi'\n",
    "\n",
    "\n",
    "OPENAI_EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "OPENAI_CHAT_MODEL = \"gpt-3.5-turbo\"\n",
    "OPENAI_CHAT_MODEL_FOR_ANALYSIS = ''\n",
    "\n",
    "FAISS_INDEX_DIMENSION = None\n",
    "RERANKER_MODEL_NAME = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "\n",
    "PRODUCT_CATALOG_JSON_PATH = \"C:\\\\Users\\\\shiva250535\\\\Downloads\\\\sample_data.json\"\n",
    "\n",
    "faiss_index = None\n",
    "product_metadata_storage = [] \n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "reranker_model = CrossEncoder(RERANKER_MODEL_NAME)\n",
    "print(f\"Re-ranker model '{RERANKER_MODEL_NAME}' loaded successfully.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54be5f68-c0a3-4e1f-85ae-d373148fadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_embedding(text: str, model: str = OPENAI_EMBEDDING_MODEL) -> list[float] | None:\n",
    "    global FAISS_INDEX_DIMENSION\n",
    "    if not client:\n",
    "        print(\"OpenAI client not initialized. Cannot get embedding.\")\n",
    "        return None\n",
    "    try:\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        response = client.embeddings.create(input=[text], model=model)\n",
    "        embedding = response.data[0].embedding\n",
    "        if FAISS_INDEX_DIMENSION is None:\n",
    "            FAISS_INDEX_DIMENSION = len(embedding)\n",
    "            print(f\"FAISS index dimension set to: {FAISS_INDEX_DIMENSION}\")\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting OpenAI embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_product_data(json_path: str) -> list[dict]:\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Successfully loaded {len(data)} products from '{json_path}'.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Product catalog file not found at '{json_path}'.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from '{json_path}'. Please check its format.\")\n",
    "        return []\n",
    "\n",
    "def build_faiss_index_and_metadata(products: list[dict]):\n",
    "    global faiss_index, product_metadata_storage, FAISS_INDEX_DIMENSION\n",
    "    \n",
    "    if not products:\n",
    "        print(\"No products to build index from.\")\n",
    "        return\n",
    "\n",
    "    print(\"Building FAISS index and metadata store...\")\n",
    "    embeddings_list = []\n",
    "    product_metadata_storage.clear()\n",
    "    if not FAISS_INDEX_DIMENSION:\n",
    "        sample_product_text_for_embedding = f\"{products[0].get('name', '')} {products[0].get('description', '')}\"\n",
    "        sample_embedding = get_openai_embedding(sample_product_text_for_embedding)\n",
    "        if not sample_embedding:\n",
    "            print(\"Error: Could not determine embedding dimension. Index building aborted.\")\n",
    "            return\n",
    "\n",
    "    for i, product in enumerate(products):\n",
    "        name = product.get(\"name\", \"\")\n",
    "        description = product.get(\"description\", \"\")\n",
    "        specs = product.get(\"specifications\", {})\n",
    "        specs_str = \"\"\n",
    "        if isinstance(specs, dict):\n",
    "            specs_str = \". \".join([f\"{k}: {v}\" for k, v in specs.items()])\n",
    "        elif isinstance(specs, str):\n",
    "            specs_str = specs\n",
    "        \n",
    "        text_to_embed = f\"Product Name: {name}. Description: {description}. Specifications: {specs_str}\"\n",
    "        \n",
    "        embedding = get_openai_embedding(text_to_embed)\n",
    "        if embedding:\n",
    "            embeddings_list.append(embedding)\n",
    "            product_metadata_storage.append(product) \n",
    "        else:\n",
    "            print(f\"Warning: Could not generate embedding for product ID {product.get('product_id', 'N/A')}. Skipping.\")\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i+1}/{len(products)} products for indexing.\")\n",
    "\n",
    "    if not embeddings_list:\n",
    "        print(\"No embeddings generated. FAISS index cannot be built.\")\n",
    "        return\n",
    "\n",
    "    embeddings_np = np.array(embeddings_list).astype('float32')\n",
    "    \n",
    "    if FAISS_INDEX_DIMENSION is None:\n",
    "        print(\"Error: FAISS_INDEX_DIMENSION is not set. Cannot build index.\")\n",
    "        return\n",
    "        \n",
    "    faiss_index = faiss.IndexFlatL2(FAISS_INDEX_DIMENSION)\n",
    "    faiss_index.add(embeddings_np)\n",
    "    print(f\"FAISS index built successfully with {faiss_index.ntotal} vectors.\")\n",
    "    print(f\"Product metadata storage populated with {len(product_metadata_storage)} items.\")\n",
    "\n",
    "\n",
    "def analyze_query_with_openai(user_query: str, model: str = OPENAI_CHAT_MODEL_FOR_ANALYSIS) -> dict:\n",
    "    if not client:\n",
    "        print(\"OpenAI client not initialized. Cannot analyze query.\")\n",
    "        return {\"category\": None, \"attributes\": [], \"raw_query\": user_query}\n",
    "        \n",
    "    system_message = \"\"\"\n",
    "    You are an AI assistant that helps analyze user queries for product recommendations.\n",
    "    Your goal is to extract the main product category the user is looking for and any key attributes or features.\n",
    "    The product categories are typically nouns like \"Laptops\", \"Smartphones\", \"Dishwashers\", \"Coffee Makers\", \"Headphones\".\n",
    "    If the user mentions multiple items, try to pick the primary one or the one that seems most like a product category.\n",
    "    Respond ONLY in JSON format with the following keys:\n",
    "    - \"category\": A string representing the identified product category (e.g., \"Laptops\", \"Smartphones\"). If no specific category is clear, return null or a very generic term like \"product\" if absolutely necessary, but try to be specific.\n",
    "    - \"attributes\": A list of strings representing key features, adjectives, or requirements mentioned by the user (e.g., [\"lightweight\", \"good battery life\", \"under $1000\", \"excellent camera\"]).\n",
    "\n",
    "    Example User Query: \"I need a lightweight laptop for travel with a good battery life, maybe under $1000\"\n",
    "    Example JSON Response:\n",
    "    {\n",
    "        \"category\": \"Laptops\",\n",
    "        \"attributes\": [\"lightweight\", \"travel\", \"good battery life\", \"under $1000\"]\n",
    "    }\n",
    "\n",
    "    User Query: \"Show me some quiet and energy efficient dishwashers for a small apartment\"\n",
    "    Example JSON Response:\n",
    "    {\n",
    "        \"category\": \"Dishwashers\",\n",
    "        \"attributes\": [\"quiet\", \"energy efficient\", \"small apartment\"]\n",
    "    }\n",
    "    \n",
    "    User Query: \"durable smartphone with excellent camera\"\n",
    "    Example JSON Response:\n",
    "    {\n",
    "        \"category\": \"Smartphones\",\n",
    "        \"attributes\": [\"durable\", \"excellent camera\"]\n",
    "    }\n",
    "    \n",
    "    User Query: \"tools for my garden\"\n",
    "    Example JSON Response:\n",
    "    {\n",
    "        \"category\": \"Gardening Tools\",\n",
    "        \"attributes\": []\n",
    "    }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_query}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        extracted_info = json.loads(content)\n",
    "        extracted_info[\"raw_query\"] = user_query\n",
    "        return extracted_info\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OpenAI query analysis: {e}\")\n",
    "        attributes = [word.lower() for word in user_query.split() if len(word) > 3]\n",
    "        return {\"category\": None, \"attributes\": attributes, \"raw_query\": user_query}\n",
    "\n",
    "\n",
    "def filter_products_by_category(extracted_category: str | None, all_metadata: list[dict]) -> list[tuple[int, dict]]:\n",
    "    if not extracted_category:\n",
    "        print(\"No category extracted, returning all products for vector search (or consider this an error).\")\n",
    "        return [(i, data) for i, data in enumerate(all_metadata)] \n",
    "\n",
    "    filtered_products_with_indices = []\n",
    "    extracted_category_lower = extracted_category.lower()\n",
    "    \n",
    "    for original_idx, product_data in enumerate(all_metadata):\n",
    "       \n",
    "        product_categories = product_data.get(\"categories\", []) \n",
    "        if isinstance(product_categories, list):\n",
    "            for cat in product_categories:\n",
    "                if extracted_category_lower in cat.lower(): \n",
    "                    filtered_products_with_indices.append((original_idx, product_data))\n",
    "                    break \n",
    "    print(f\"Metadata filtering: Extracted category '{extracted_category}'. Found {len(filtered_products_with_indices)} matching products.\")\n",
    "    return filtered_products_with_indices\n",
    "\n",
    "\n",
    "def search_faiss_on_filtered_subset(query_embedding_np: np.ndarray,\n",
    "                                     original_faiss_index: faiss.Index,\n",
    "                                     filtered_product_indices: list[int],\n",
    "                                     top_k: int) -> tuple[list[float], list[int]]:\n",
    "\n",
    "    if not filtered_product_indices:\n",
    "        return [], []\n",
    "\n",
    "    if not original_faiss_index or original_faiss_index.ntotal == 0:\n",
    "        print(\"Original FAISS index is not available or empty.\")\n",
    "        return [],[]\n",
    "\n",
    "    num_filtered = len(filtered_product_indices)\n",
    "    \n",
    "    valid_filtered_indices = [idx for idx in filtered_product_indices if 0 <= idx < original_faiss_index.ntotal]\n",
    "    if not valid_filtered_indices:\n",
    "        print(\"No valid indices after filtering. Cannot search.\")\n",
    "        return [], []\n",
    "\n",
    "    try:\n",
    "       \n",
    "        filtered_vectors_list = []\n",
    "        for idx in valid_filtered_indices:\n",
    "            try:\n",
    "                vec = original_faiss_index.reconstruct(idx)\n",
    "                filtered_vectors_list.append(vec)\n",
    "            except Exception as e_reconstruct:\n",
    "                print(f\"Warning: Could not reconstruct vector for index {idx}. Error: {e_reconstruct}. Skipping this vector.\")\n",
    "        \n",
    "        if not filtered_vectors_list:\n",
    "            print(\"No vectors could be reconstructed for the filtered set.\")\n",
    "            return [], []\n",
    "            \n",
    "        filtered_vectors_np = np.array(filtered_vectors_list).astype('float32')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reconstructing vectors for FAISS sub-search: {e}\")\n",
    "        return [], []\n",
    "\n",
    "    if filtered_vectors_np.shape[0] == 0:\n",
    "        print(\"No vectors in the filtered subset for FAISS search.\")\n",
    "        return [], []\n",
    "\n",
    "    temp_faiss_index = faiss.IndexFlatL2(FAISS_INDEX_DIMENSION)\n",
    "    temp_faiss_index.add(filtered_vectors_np)\n",
    "\n",
    "    actual_top_k = min(top_k, temp_faiss_index.ntotal)\n",
    "    if actual_top_k == 0:\n",
    "        return [], []\n",
    "        \n",
    "    distances, temp_indices = temp_faiss_index.search(query_embedding_np, actual_top_k)\n",
    "\n",
    "    original_indices_of_results = [valid_filtered_indices[i] for i in temp_indices[0]]\n",
    "    \n",
    "    return distances[0].tolist(), original_indices_of_results\n",
    "\n",
    "\n",
    "def rerank_products(query: str, products_to_rerank: list[dict], top_n: int = 5) -> list[dict]:\n",
    "\n",
    "    if not reranker_model:\n",
    "        print(\"Re-ranker model not loaded. Skipping re-ranking.\")\n",
    "        return products_to_rerank[:top_n]\n",
    "\n",
    "    if not products_to_rerank:\n",
    "        return []\n",
    "\n",
    "    pairs = []\n",
    "    for product in products_to_rerank:\n",
    "        prod_name = product.get(\"name\", \"\")\n",
    "        prod_desc = product.get(\"description\", \"\")\n",
    "        prod_specs = product.get(\"specifications\", {})\n",
    "        specs_str = \"\"\n",
    "        if isinstance(prod_specs, dict):\n",
    "            specs_str = \", \".join([f\"{k}: {v}\" for k,v in list(prod_specs.items())[:3]]) # Top 3 specs\n",
    "        elif isinstance(prod_specs, str):\n",
    "            specs_str = prod_specs[:100]\n",
    "        \n",
    "        product_text = f\"{prod_name}. {prod_desc}. Key Specs: {specs_str}\"\n",
    "        pairs.append([query, product_text])\n",
    "\n",
    "    if not pairs:\n",
    "        return []\n",
    "\n",
    "    print(f\"Re-ranking {len(pairs)} product pairs...\")\n",
    "    scores = reranker_model.predict(pairs)\n",
    "\n",
    "    for i, product in enumerate(products_to_rerank):\n",
    "        product['rerank_score'] = scores[i]\n",
    "\n",
    "    reranked_products = sorted(products_to_rerank, key=lambda x: x.get('rerank_score', -float('inf')), reverse=True)\n",
    "    \n",
    "    return reranked_products[:top_n]\n",
    "\n",
    "\n",
    "def generate_recommendation_with_openai(user_query: str, ranked_products: list[dict], model: str = OPENAI_CHAT_MODEL) -> str:\n",
    "    \"\"\"Generates a natural language recommendation using OpenAI.\"\"\"\n",
    "    if not client:\n",
    "        return \"OpenAI client not initialized. Cannot generate recommendation.\"\n",
    "        \n",
    "    if not ranked_products:\n",
    "        return \"I couldn't find any specific products matching your refined query after filtering and searching. Could you try rephrasing or being more general?\"\n",
    "\n",
    "    context = \"Based on your query and our product catalog, here are some recommendations:\\n\\n\"\n",
    "    for i, prod in enumerate(ranked_products):\n",
    "        context += f\"Product {i+1}:\\n\"\n",
    "        context += f\"  ID: {prod.get('product_id', 'N/A')}\\n\"\n",
    "        context += f\"  Name: {prod.get('name', 'N/A')}\\n\"\n",
    "        context += f\"  Price: ${prod.get('price', 'N/A')}\\n\"\n",
    "        categories = prod.get('categories', [])\n",
    "        if isinstance(categories, list) and categories:\n",
    "            context += f\"  Category: {', '.join(categories)}\\n\"\n",
    "        \n",
    "        specs = prod.get(\"specifications\")\n",
    "        specs_summary = \"Not available\"\n",
    "        if isinstance(specs, dict) and specs:\n",
    "            specs_summary = \", \".join([f\"{k}: {v}\" for k, v in list(specs.items())[:4]]) # First 4 specs\n",
    "        elif isinstance(specs, str) and specs:\n",
    "            specs_summary = specs[:150] + \"...\" if len(specs) > 150 else specs\n",
    "        context += f\"  Key Specifications: {specs_summary}\\n\"\n",
    "        if 'rerank_score' in prod:\n",
    "            context += f\"  Relevance Score (Re-ranker): {prod['rerank_score']:.4f}\\n\"\n",
    "        elif 'faiss_distance' in prod:\n",
    "            context += f\"  Relevance Score (FAISS Distance - lower is better): {prod['faiss_distance']:.4f}\\n\"\n",
    "        context += \"\\n\"\n",
    "        \n",
    "    system_message = \"\"\"\n",
    "    You are a helpful AI product recommendation assistant.\n",
    "    Based on the user's query and the provided product information (which has been retrieved and ranked), generate a friendly and helpful recommendation.\n",
    "    Explain briefly why certain products are a good fit. If multiple products are good, you can mention them.\n",
    "    If the retrieved products seem like a mixed bag, try to pick the best ones or explain the trade-offs.\n",
    "    Be concise but informative. Do not refer to yourself as an AI model. Just provide the recommendation.\n",
    "    Focus on the products provided in the context.\n",
    "    \"\"\"\n",
    "    prompt_messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": f\"My query is: \\\"{user_query}\\\"\\n\\nHere's the product information I found:\\n{context}\"}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=prompt_messages,\n",
    "            temperature=0.7,\n",
    "            max_tokens=700\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OpenAI recommendation generation: {e}\")\n",
    "        return \"I encountered an issue while trying to generate the recommendation.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d2f8887-ff03-41a1-bbbe-961946b2f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global faiss_index\n",
    "\n",
    "    products = load_product_data(PRODUCT_CATALOG_JSON_PATH)\n",
    "    if not products:\n",
    "        print(\"Exiting due to no product data.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    build_faiss_index_and_metadata(products)\n",
    "    if faiss_index is None or faiss_index.ntotal == 0:\n",
    "        print(\"FAISS index not built or empty. Exiting.\")\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10b27d9c-de2b-4f17-a9c5-a7c3ffe3d1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 500 products from 'C:\\Users\\shiva250535\\Downloads\\sample_data.json'.\n",
      "Building FAISS index and metadata store...\n",
      "Error getting OpenAI embedding: Error code: 401 - {'error': {'message': 'Incorrect API key provided: knvh5u5r******77gi. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "Error: Could not determine embedding dimension. Index building aborted.\n",
      "FAISS index not built or empty. Exiting.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if OPENAI_API_KEY and client:\n",
    "        main()\n",
    "    else:\n",
    "        print(\"OpenAI API key not configured or client initialization failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec02258-6869-4d9e-a076-9d7f73040a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0a384-dd62-41bf-b0d3-a64e4155dd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
